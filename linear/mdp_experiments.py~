import numpy as np
import pdb

class Agent():
    def __init__(self, num_actions, policy_features, value_features,
                 policy_stepsize, critic_stepsize, nstep, gamma,
                 FLAG_BASELINE, FLAG_PG_TYPE='regular',
                 policy_weight_init_left=0, policy_weight_init_right=0,
                 env=None, start_state=None, FLAG_POPULAR_PG=False,
                 value_weight_init=0, grad_bias=None, grad_noise=None,
                 tilecoder=None):
        self.policy_features = policy_features
        self.value_features = value_features
        self.num_actions = num_actions

        self.policy_weight = np.zeros((policy_features.shape[1], num_actions))
        if num_actions == 2:
            self.policy_weight[:, 0] = policy_weight_init_left
            self.policy_weight[:, 1] = policy_weight_init_right
        if num_actions > 2:
            self.policy_weight[:, :] = policy_weight_init_left
            self.policy_weight[:, -1] = policy_weight_init_right

        self.value_weight_init = value_weight_init
        if value_features is None:
            self.value_weight = None
        else:
            self.value_weight \
                = self.value_weight_init * np.ones((value_features.shape[1], 1))

        self.policy_stepsize = policy_stepsize
        self.critic_stepsize = critic_stepsize

        self.FLAG_BASELINE = FLAG_BASELINE
        self.FLAG_POPULAR_PG = FLAG_POPULAR_PG
        self.FLAG_PG_TYPE = FLAG_PG_TYPE
        self.gamma = gamma
        self.nstep = nstep

        self.pi = None
        self.FLAG_POLICY_UPDATED = True

        self.env = env
        self.start_state = start_state

        self.grad_bias = grad_bias
        self.grad_noise = grad_noise

        self.tilecoder = tilecoder

    @staticmethod
    def softmax(x):
        e_x = np.exp(x - np.max(x, 1).reshape(-1, 1))
        out = e_x / e_x.sum(1).reshape(-1, 1)
        return out

    def get_state_features(state):
        feat = np.array(self.tilecoder.encode(state)).reshape(-1, 1)

    def get_state_features_indices(state):
        return self.tilecoder.get_indices(state)

    def take_action(self, state):
        if self.FLAG_POLICY_UPDATED:
            theta = np.matmul(self.policy_features, self.policy_weight)
            self.pi = self.softmax(theta)
            self.FLAG_POLICY_UPDATED = False
        action = np.random.choice(self.num_actions, p=self.pi[state])
        return action, self.pi[state, action]

    def pred_v_pi(self, state):
        idx = self.get_state_features_indices(state)
        v_pred = self.value_weight[idx].sum()
        # v_pred = np.matmul(self.value_features[state], self.value_weight)
        return v_pred

    def update_value_fn(self, state, delta):
        self.value_weight = self.value_weight \
            + self.critic_stepsize * delta \
            * self.value_features[state].reshape(self.value_weight.shape)

    def calc_grad_log_pi(self, state, action):
        x = self.policy_features[state].reshape(-1, 1)
        theta = np.matmul(x.T, self.policy_weight)
        pi = self.softmax(theta).T

        I_action = np.zeros((self.num_actions, 1))
        I_action[action] = 1

        one_vec = np.ones((1, self.num_actions))

        if self.FLAG_PG_TYPE == 'regular':
            grad = np.matmul(x, (I_action - pi).T) # equivalent
        elif self.FLAG_PG_TYPE == 'alternate':
            grad = np.matmul(x, I_action.T) # equivalent

        return grad
    
    def update_policy(self, state, action, delta, discounting_term):
        policy_grad = discounting_term * delta \
            * self.calc_grad_log_pi(state, action)
        
        if self.grad_bias is not None or self.grad_noise is not None:
            policy_grad += np.random.normal(loc=self.grad_bias,
                                            scale=self.grad_noise,
                                            size=policy_grad.shape)
        
        self.policy_weight += self.policy_stepsize * policy_grad
        self.FLAG_POLICY_UPDATED = True

def run_experiment(env_name, num_runs, num_episodes,
                   P, r, start_state, terminal_states,
                   num_actions, policy_features, value_features,
                   policy_stepsize, critic_stepsize, nstep, gamma,
                   FLAG_BASELINE, FLAG_LEARN_VPI, FLAG_PG_TYPE,
                   reward_noise=0, vpi_bias=0,
                   policy_weight_init_left=0, policy_weight_init_right=0,
                   value_weight_init=0, episode_cutoff_length=1000,
                   grad_bias=None, grad_noise=None, FLAG_FIXED_VPI_OPTIM=False):
    return_across_runs = []
    ep_len_across_runs = []
    vpi_across_runs = []

    for run in range(num_runs):
        np.random.seed(run)

        # define the agent and the environment
        env = LinearChain(P=P, r=r, start_state=start_state,
                          terminal_states=terminal_states,
                          reward_noise=reward_noise,
                          episode_cutoff_length=episode_cutoff_length)

        agent = Agent(num_actions=num_actions,
                      policy_features=policy_features,
                      value_features=value_features,
                      policy_stepsize=policy_stepsize,
                      critic_stepsize=critic_stepsize,
                      nstep=nstep, gamma=gamma,
                      FLAG_BASELINE=FLAG_BASELINE,
                      FLAG_PG_TYPE=FLAG_PG_TYPE,
                      policy_weight_init_left=policy_weight_init_left,
                      policy_weight_init_right=policy_weight_init_right,
                      env=env, start_state=start_state,
                      FLAG_POPULAR_PG=False,
                      value_weight_init=value_weight_init,
                      grad_bias=grad_bias, grad_noise=grad_noise)

        return_across_episodes = []
        ep_len_across_episodes = []
        vpi_across_episodes = []

        episode_i = 0
        while episode_i < num_episodes:
            episode_i += 1

            if FLAG_PG_TYPE not in ['regular', 'alternate']:
                raise NotImplementedError()
            traj = {'state_list': [],
                    'action_list': [],
                    'action_prob_list': [],
                    'reward_list': [],
                    'next_state_list': []}

            # sample a trajectory by following the current policy
            discounting_term = 1
            done = False
            state = env.start() # state = env.reset()
            while not done:
                action, action_prob = agent.take_action(state)
                reward, next_state, done = env.step(action=action)
                # next_state, reward, done, _ = env.step(action)

                traj['state_list'].append(state)
                traj['action_list'].append(action)
                traj['action_prob_list'].append(action_prob)
                traj['reward_list'].append(reward)
                traj['next_state_list'].append(next_state)

                v_pi_pred = agent.pred_v_pi()
                delta = reward + self.gamma * v_pi_pred[next_state] \
                    - v_pi_pred[state]

                # update the policy
                agent.update_policy(state, action, delta, discounting_term)
                # update the value function
                if FLAG_LEARN_VPI:
                    agent.update_value_fn(state, delta)

                discounting_term *= gamma
                state = next_state

            # update the policy
            expected_ret = agent.update_policy(traj, v_pi)

            if np.isnan(agent.policy_weight).any() == True:
                # if policy is nan, kill the training and give bad return
                tmp = num_episodes - episode_i + 1
                return_across_episodes += [-1/(1 - gamma)] * tmp
                vpi_across_episodes += [-1/(1 - gamma)] * tmp
                ep_len_across_episodes += [1] * tmp
                break
            else:
                # update the value function
                if FLAG_LEARN_VPI:
                    agent.update_value_fn(traj)

                # save the returns for this episode
                factor = 1
                discounted_return = 0
                for reward in traj['reward_list']:
                    discounted_return += factor * reward
                    factor *= gamma

                return_across_episodes.append(discounted_return)
                vpi_across_episodes.append(expected_ret)
                ep_len_across_episodes.append(len(traj['reward_list']))

        return_across_runs.append(return_across_episodes)
        vpi_across_runs.append(vpi_across_episodes)
        ep_len_across_runs.append(ep_len_across_episodes)

    dat = {'returns': return_across_runs,
           'ep_len': ep_len_across_runs,
           'vpi_s0': vpi_across_runs}

    return dat
